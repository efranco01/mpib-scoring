{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT:\n",
    "# TODO: Build simple ui or just use terminal for inputs\n",
    "# TODO: Allow for more in-depth customization in args (argparse)\n",
    "\n",
    "# SCORING:\n",
    "# TODO: Score distance using city block method (will be in the nitty gritty area)\n",
    "# TODO: Score avg distance across trials (will be in the nitty gritty area)\n",
    "# TODO: The PRACT subset only has 3 instead of 4 inputs! (fix this)\n",
    "\n",
    "# OUTPUT:\n",
    "# TODO: Display more in-depth details further along (horizontal)\n",
    "# TODO: Display avg distance from correct block (incorrect answers) in the \n",
    "# TODO: Print LOGS stating what is being done\n",
    "\n",
    "# FINAL:\n",
    "# TODO: Migrate .ipynb to .py format and finish syncing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plot\n",
    "import os\n",
    "import logging as log\n",
    "import shutil\n",
    "\n",
    "class Util:\n",
    "    def __init__(self) -> None:\n",
    "        self.currentDir = os.getcwd()\n",
    "\n",
    "    def findFile(self, testName=str):\n",
    "\n",
    "        # Search for a txt file in the current directory\n",
    "        files_found = []\n",
    "        for file in os.listdir(self.currentDir):\n",
    "            if file.endswith('.txt') and testName in file:\n",
    "                files_found.append(file)\n",
    "\n",
    "        if len(files_found) == 0:\n",
    "            print(f'No TXT file named {testName} found in current directory')\n",
    "            return None\n",
    "        elif len(files_found) == 1:\n",
    "            # If a TXT file is found, read it into a Pandas dataframe\n",
    "            df = pd.read_csv(os.path.join(self.currentDir, files_found[0]), delimiter='\\t')\n",
    "            print(f'Successfully read file: {files_found[0]}')\n",
    "            return df\n",
    "        else:\n",
    "            print(f'{len(files_found)} files with the name {testName} found in current directory:')\n",
    "            for i, file in enumerate(files_found):\n",
    "                print(f'{i+1}. {file}')\n",
    "            selection = input('Please select the file you want to read (enter number): ')\n",
    "            try:\n",
    "                selection = int(selection)\n",
    "            except:\n",
    "                print('Invalid input, please enter a number.')\n",
    "                return None\n",
    "            if selection > 0 and selection <= len(files_found):\n",
    "                df = pd.read_csv(os.path.join(currentDir, files_found[selection-1]), delimiter='\\t')\n",
    "                print(f'Successfully read file: {files_found[selection-1]}')\n",
    "                return df\n",
    "            else:\n",
    "                print(f'Invalid selection: {selection}. Please enter a number between 1 and {len(files_found)}.')\n",
    "                return None\n",
    "\n",
    "    def getTrialType(self, df=pd.DataFrame, trialLoc=4):\n",
    "        \n",
    "        # List different types of trials in the dataframe\n",
    "        trialTypes = df.iloc[:, trialLoc].unique()\n",
    "        \n",
    "        return trialTypes\n",
    "    \n",
    "    def subsetByTrial(self, df=pd.DataFrame, trialTypes=np.ndarray, trialLoc=4):\n",
    "        \n",
    "        # Create a list to hold the subsetted dataframes\n",
    "        subsetList = []\n",
    "        \n",
    "        # Subset data based on trial number\n",
    "        for trial in trialTypes:\n",
    "            # Create a new dataframe for each trial type\n",
    "            trialDf = df[df.iloc[:, trialLoc] == trial]\n",
    "            \n",
    "            # Add each dataframe to a list\n",
    "            subsetList.append(trialDf)\n",
    "        \n",
    "        # Return the list of dataframes\n",
    "        return subsetList\n",
    "\n",
    "    def makeScoredDir(self):\n",
    "\n",
    "        # Create a new directory for the scored files\n",
    "        newDir = os.path.join(self.currentDir, 'scored')\n",
    "        if not os.path.exists(newDir):\n",
    "            os.mkdir(newDir)\n",
    "            print(f'Successfully created directory: {newDir}')\n",
    "        else:\n",
    "            print(f'Directory {newDir} already exists')\n",
    "        \n",
    "        # Move all files with 'scores' in their name to the new scored directory\n",
    "        for file in os.listdir(self.currentDir):\n",
    "            if 'scores' in file:\n",
    "                filePath = os.path.join(self.currentDir, file)\n",
    "                shutil.move(filePath, newDir)\n",
    "                print(f'Successfully moved {file} to {newDir}')\n",
    "\n",
    "class Main(Util):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "myUtil = Util()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: 1001_FiguralSpeed.txt\n",
      "  Trial Type  Proportion Correct  Total Inputs  Average Response Time  \\\n",
      "0    PRACT              0.948718            39            2245.512821   \n",
      "1    TRIAL1             0.950000            40            2837.050000   \n",
      "2    TRIAL2             0.900000            40            2558.875000   \n",
      "\n",
      "   Average Response Time (Correct)  Average Response Time (Incorrect)  \\\n",
      "0                      2222.513514                             2671.0   \n",
      "1                      2838.605263                             2807.5   \n",
      "2                      2557.805556                             2568.5   \n",
      "\n",
      "   Median Response Time  Median Response Time (Correct)  \\\n",
      "0                2492.0                          2492.0   \n",
      "1                2604.0                          2604.0   \n",
      "2                2456.5                          2456.5   \n",
      "\n",
      "   Median Response Time (Incorrect)  \n",
      "0                            2671.0  \n",
      "1                            2807.5  \n",
      "2                            2521.0  \n"
     ]
    }
   ],
   "source": [
    "def fsScore():\n",
    "    # grab test document\n",
    "    fs = myUtil.findFile('FiguralSpeed')\n",
    "\n",
    "    # determine all unique trials\n",
    "    trials = myUtil.getTrialType(fs)\n",
    "\n",
    "    # subset data by trial\n",
    "    trialDfs = myUtil.subsetByTrial(fs, trials)\n",
    "\n",
    "    # create a list to store the results\n",
    "    trialData = []\n",
    "\n",
    "    # score each trial and add results to the list\n",
    "    for trial in trialDfs:\n",
    "        trialType = trial.iloc[0, 4]\n",
    "        pc = trial.iloc[:, 13].mean()\n",
    "        responseTimes = trial.iloc[:, 14].mean()\n",
    "        responseCorrect = trial[trial.iloc[:, 13] == 1].iloc[:, 14].mean()\n",
    "        responseIncorrect = trial[trial.iloc[:, 13] == 0].iloc[:, 14].mean()\n",
    "        medianResponseTime = trial.iloc[:, 14].median()\n",
    "        medianResponseCorrect = trial[trial.iloc[:, 13] == 1].iloc[:, 14].median()\n",
    "        medianResponseIncorrect = trial[trial.iloc[:, 13] == 0].iloc[:, 14].median()\n",
    "\n",
    "        trialData.append({\n",
    "            'Trial Type': trialType,\n",
    "            'Proportion Correct': pc,\n",
    "            'Total Inputs': trial.iloc[:, 13].count(),\n",
    "            'Average Response Time': responseTimes,\n",
    "            'Average Response Time (Correct)': responseCorrect,\n",
    "            'Average Response Time (Incorrect)': responseIncorrect,\n",
    "            'Median Response Time': medianResponseTime,\n",
    "            'Median Response Time (Correct)': medianResponseCorrect,\n",
    "            'Median Response Time (Incorrect)': medianResponseIncorrect\n",
    "        })\n",
    "\n",
    "    # create a data frame from the results list\n",
    "    df = pd.DataFrame(trialData)\n",
    "\n",
    "    # save the data frame to a CSV file\n",
    "    df.to_csv('fs_scores.csv', float_format='%.2f', index=False)\n",
    "\n",
    "    # print the data frame\n",
    "    print(df)\n",
    "\n",
    "fsScore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: 1001_LetterUpdating.txt\n",
      "     Trial  Proportion Correct  Total Inputs\n",
      "0  PRACT              0.666667             3\n",
      "1  TRIAL1             0.875000             8\n",
      "2  TRIAL2             0.666667             8\n"
     ]
    }
   ],
   "source": [
    "def luScore():\n",
    "    # grab test document\n",
    "    lu = myUtil.findFile('LetterUpdating')\n",
    "\n",
    "    # determine all unique trials\n",
    "    trials = myUtil.getTrialType(lu)\n",
    "\n",
    "    # subset data by trial\n",
    "    trialDfs = myUtil.subsetByTrial(lu, trials)\n",
    "\n",
    "    # create an empty list to store trial data\n",
    "    trialData = []\n",
    "\n",
    "    # score each trial (this will be general scores)\n",
    "    for trial in trialDfs:\n",
    "        trialName = trial.iloc[0, 4]\n",
    "        pc = trial.iloc[:, 18].mean() / 3\n",
    "        trialData.append([trialName, pc, trial.iloc[:, 18].count()])\n",
    "\n",
    "    # create a pandas DataFrame to store trial data\n",
    "    columnNames = ['Trial', 'Proportion Correct', 'Total Inputs']\n",
    "    df = pd.DataFrame(trialData, columns=columnNames)\n",
    "\n",
    "    # save the data frame to a CSV file\n",
    "    df.to_csv('lu_scores.csv', float_format='%.2f', index=False)\n",
    "\n",
    "    # output the DataFrame\n",
    "    print(df)\n",
    "\n",
    "luScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: 1001_MotoricSpeed.txt\n",
      "\n",
      "Scoring PRACT1\n",
      "Proportion correct: 59.09% with total inputs: 22\n",
      "Average response time: 269.23\n",
      "Average response time for correct answers: 423.69\n",
      "Average response time for incorrect answers: 46.11\n",
      "Median response time: 307.50\n",
      "Median response time for correct answers: 354.00\n",
      "Median response time for incorrect answers: 0.00\n",
      "\n",
      "Scoring 5\n",
      "Proportion correct: nan% with total inputs: 0\n",
      "Average response time: nan\n",
      "Average response time for correct answers: nan\n",
      "Average response time for incorrect answers: nan\n",
      "Median response time: nan\n",
      "Median response time for correct answers: nan\n",
      "Median response time for incorrect answers: nan\n",
      "\n",
      "Scoring PRACT2\n",
      "Proportion correct: 100.00% with total inputs: 24\n",
      "Average response time: 318.88\n",
      "Average response time for correct answers: 318.88\n",
      "Average response time for incorrect answers: nan\n",
      "Median response time: 303.00\n",
      "Median response time for correct answers: 303.00\n",
      "Median response time for incorrect answers: nan\n",
      "\n",
      "Scoring TRIAL1\n",
      "Proportion correct: 95.83% with total inputs: 48\n",
      "Average response time: 294.96\n",
      "Average response time for correct answers: 297.48\n",
      "Average response time for incorrect answers: 237.00\n",
      "Median response time: 281.00\n",
      "Median response time for correct answers: 285.50\n",
      "Median response time for incorrect answers: 237.00\n",
      "\n",
      "Scoring TRIAL2\n",
      "Proportion correct: 100.00% with total inputs: 48\n",
      "Average response time: 321.77\n",
      "Average response time for correct answers: 321.77\n",
      "Average response time for incorrect answers: nan\n",
      "Median response time: 311.50\n",
      "Median response time for correct answers: 311.50\n",
      "Median response time for incorrect answers: nan\n",
      "\n",
      "Scoring TRIAL3\n",
      "Proportion correct: 95.83% with total inputs: 48\n",
      "Average response time: 380.62\n",
      "Average response time for correct answers: 393.61\n",
      "Average response time for incorrect answers: 82.00\n",
      "Median response time: 371.50\n",
      "Median response time for correct answers: 372.50\n",
      "Median response time for incorrect answers: 82.00\n",
      "\n",
      "Scoring TRIAL4\n",
      "Proportion correct: 100.00% with total inputs: 48\n",
      "Average response time: 367.44\n",
      "Average response time for correct answers: 367.44\n",
      "Average response time for incorrect answers: nan\n",
      "Median response time: 331.00\n",
      "Median response time for correct answers: 331.00\n",
      "Median response time for incorrect answers: nan\n",
      "\n",
      "Scoring TRIAL5\n",
      "Proportion correct: 97.92% with total inputs: 48\n",
      "Average response time: 321.44\n",
      "Average response time for correct answers: 321.85\n",
      "Average response time for incorrect answers: 302.00\n",
      "Median response time: 312.50\n",
      "Median response time for correct answers: 314.00\n",
      "Median response time for incorrect answers: 302.00\n",
      "\n",
      "Scoring TRIAL6\n",
      "Proportion correct: 95.83% with total inputs: 48\n",
      "Average response time: 304.77\n",
      "Average response time for correct answers: 305.76\n",
      "Average response time for incorrect answers: 282.00\n",
      "Median response time: 302.00\n",
      "Median response time for correct answers: 303.00\n",
      "Median response time for incorrect answers: 282.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/numpy/lib/nanfunctions.py:1215: RuntimeWarning: Mean of empty slice\n",
      "  return np.nanmean(a, axis, out=out, keepdims=keepdims)\n"
     ]
    }
   ],
   "source": [
    "def msScore():\n",
    "    \n",
    "    # grab test document\n",
    "    ms = myUtil.findFile('MotoricSpeed')\n",
    "\n",
    "    # determine all unique trials\n",
    "    trials = myUtil.getTrialType(ms, 5)\n",
    "\n",
    "    # subset data by trial\n",
    "    trialDfs = myUtil.subsetByTrial(ms, trials, 5)\n",
    "\n",
    "    # score each trial (this will be general scores)\n",
    "    for trial in trialDfs:\n",
    "        print(f'\\nScoring {trial.iloc[0, 5]}')\n",
    "        pc = trial.iloc[:, 15].mean()\n",
    "        print(f'Proportion correct: {pc:.2%} with total inputs: {trial.iloc[:, 13].count()}')\n",
    "        responseTimes = trial.iloc[:, 14].mean()\n",
    "        print(f'Average response time: {responseTimes:.2f}')\n",
    "        responseCorrect = trial[trial.iloc[:, 15] == 1].iloc[:, 14].mean()\n",
    "        print(f'Average response time for correct answers: {responseCorrect:.2f}')\n",
    "        responseIncorrect = trial[trial.iloc[:, 15] == 0].iloc[:, 14].mean()\n",
    "        print(f'Average response time for incorrect answers: {responseIncorrect:.2f}')\n",
    "        medianResponseTime = trial.iloc[:, 14].median()\n",
    "        print(f'Median response time: {medianResponseTime:.2f}')\n",
    "        medianResponseCorrect = trial[trial.iloc[:, 15] == 1].iloc[:, 14].median()\n",
    "        print(f'Median response time for correct answers: {medianResponseCorrect:.2f}')\n",
    "        medianResponseIncorrect = trial[trial.iloc[:, 15] == 0].iloc[:, 14].median()\n",
    "        print(f'Median response time for incorrect answers: {medianResponseIncorrect:.2f}')\n",
    "\n",
    "msScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: 1001_NumberMemory.txt\n",
      "\n",
      "Scoring PRACT1 \n",
      "Proportion correct: 22.22% with total inputs: 9\n",
      "Average response time: 4411.22\n",
      "Average response time for correct answers: 3969.50\n",
      "Average response time for incorrect answers: 4537.43\n",
      "Median response time: 3594.00\n",
      "Median response time for correct answers: 3969.50\n",
      "Median response time for incorrect answers: 3252.00\n",
      "\n",
      "Scoring TRIAL1 \n",
      "Proportion correct: 20.00% with total inputs: 10\n",
      "Average response time: 3808.90\n",
      "Average response time for correct answers: 1567.00\n",
      "Average response time for incorrect answers: 4369.38\n",
      "Median response time: 3932.00\n",
      "Median response time for correct answers: 1567.00\n",
      "Median response time for incorrect answers: 4055.00\n",
      "\n",
      "Scoring TRIAL2 \n",
      "Proportion correct: 30.00% with total inputs: 10\n",
      "Average response time: 2808.70\n",
      "Average response time for correct answers: 2510.67\n",
      "Average response time for incorrect answers: 2936.43\n",
      "Median response time: 3013.00\n",
      "Median response time for correct answers: 1788.00\n",
      "Median response time for incorrect answers: 3184.00\n"
     ]
    }
   ],
   "source": [
    "def nmScore():\n",
    "    \n",
    "    # grab test document\n",
    "    nm = myUtil.findFile('NumberMemory')\n",
    "\n",
    "    # determine all unique trials\n",
    "    trials = myUtil.getTrialType(nm, 4)\n",
    "\n",
    "    # subset data by trial\n",
    "    trialDfs = myUtil.subsetByTrial(nm, trials)\n",
    "\n",
    "    # score each trial (this will be general scores)\n",
    "    for trial in trialDfs:\n",
    "        print(f'\\nScoring {trial.iloc[0, 4]}')\n",
    "        pc = trial.iloc[:, 10].mean()\n",
    "        print(f'Proportion correct: {pc:.2%} with total inputs: {trial.iloc[:, 13].count()}')\n",
    "        responseTimes = trial.iloc[:, 11].mean()\n",
    "        print(f'Average response time: {responseTimes:.2f}')\n",
    "        responseCorrect = trial[trial.iloc[:, 10] == 1].iloc[:, 11].mean()\n",
    "        print(f'Average response time for correct answers: {responseCorrect:.2f}')\n",
    "        responseIncorrect = trial[trial.iloc[:, 10] == 0].iloc[:, 11].mean()\n",
    "        print(f'Average response time for incorrect answers: {responseIncorrect:.2f}')\n",
    "        medianResponseTime = trial.iloc[:, 11].median()\n",
    "        print(f'Median response time: {medianResponseTime:.2f}')\n",
    "        medianResponseCorrect = trial[trial.iloc[:, 10] == 1].iloc[:, 11].median()\n",
    "        print(f'Median response time for correct answers: {medianResponseCorrect:.2f}')\n",
    "        medianResponseIncorrect = trial[trial.iloc[:, 10] == 0].iloc[:, 11].median()\n",
    "        print(f'Median response time for incorrect answers: {medianResponseIncorrect:.2f}')\n",
    "\n",
    "nmScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: 1001_Numerical_nBack.txt\n",
      "['PRACT_1  ' 'PRACT_2  ' 'TRIAL1_1 ' 'TRIAL1_2 ' 'TRIAL2_1 ' 'TRIAL2_2 ']\n",
      "\n",
      "Scoring PRACT_1  \n",
      "Proportion correct: 58.62% (total correct = 17) with total inputs: 29\n",
      "Average response time: 484.14\n",
      "Average response time for correct answers: 825.88\n",
      "Average response time for incorrect answers: 0.00\n",
      "Median response time: 700.00\n",
      "Median response time for correct answers: 785.00\n",
      "Median response time for incorrect answers: 0.00\n",
      "\n",
      "Scoring PRACT_2  \n",
      "Proportion correct: 86.67% (total correct = 26) with total inputs: 30\n",
      "Average response time: 672.67\n",
      "Average response time for correct answers: 729.31\n",
      "Average response time for incorrect answers: 304.50\n",
      "Median response time: 679.00\n",
      "Median response time for correct answers: 699.50\n",
      "Median response time for incorrect answers: 0.00\n",
      "\n",
      "Scoring TRIAL1_1 \n",
      "Proportion correct: 86.67% (total correct = 26) with total inputs: 30\n",
      "Average response time: 649.17\n",
      "Average response time for correct answers: 749.04\n",
      "Average response time for incorrect answers: 0.00\n",
      "Median response time: 677.50\n",
      "Median response time for correct answers: 695.50\n",
      "Median response time for incorrect answers: 0.00\n",
      "\n",
      "Scoring TRIAL1_2 \n",
      "Proportion correct: 90.00% (total correct = 27) with total inputs: 30\n",
      "Average response time: 605.60\n",
      "Average response time for correct answers: 672.89\n",
      "Average response time for incorrect answers: 0.00\n",
      "Median response time: 620.50\n",
      "Median response time for correct answers: 646.00\n",
      "Median response time for incorrect answers: 0.00\n",
      "\n",
      "Scoring TRIAL2_1 \n",
      "Proportion correct: 90.00% (total correct = 27) with total inputs: 30\n",
      "Average response time: 624.67\n",
      "Average response time for correct answers: 694.07\n",
      "Average response time for incorrect answers: 0.00\n",
      "Median response time: 639.50\n",
      "Median response time for correct answers: 661.00\n",
      "Median response time for incorrect answers: 0.00\n",
      "\n",
      "Scoring TRIAL2_2 \n",
      "Proportion correct: 73.33% (total correct = 22) with total inputs: 30\n",
      "Average response time: 644.07\n",
      "Average response time for correct answers: 716.00\n",
      "Average response time for incorrect answers: 446.25\n",
      "Median response time: 658.00\n",
      "Median response time for correct answers: 675.50\n",
      "Median response time for incorrect answers: 248.00\n"
     ]
    }
   ],
   "source": [
    "# this test has a different trial format and the column is 6 instead of 5\n",
    "def nbScore():\n",
    "    \n",
    "    # grab test document\n",
    "    nb = myUtil.findFile('Numerical_nBack')\n",
    "\n",
    "    # determine all unique trials\n",
    "    trials = myUtil.getTrialType(nb, 5)\n",
    "    print(trials)\n",
    "\n",
    "    # subset data by trial\n",
    "    trialDfs = myUtil.subsetByTrial(nb, trials, 5)\n",
    "\n",
    "    # score each trial (this will be general scores)\n",
    "    for trial in trialDfs:\n",
    "        print(f'\\nScoring {trial.iloc[0, 5]}')\n",
    "        pc = trial.iloc[:, 13].mean()\n",
    "        totalCorrect = trial.iloc[:, 13].sum()\n",
    "        print(f'Proportion correct: {pc:.2%} (total correct = {totalCorrect}) with total inputs: {trial.iloc[:, 13].count()}')\n",
    "        responseTimes = trial.iloc[:, 14].mean()\n",
    "        print(f'Average response time: {responseTimes:.2f}')\n",
    "        responseCorrect = trial[trial.iloc[:, 13] == 1].iloc[:, 14].mean()\n",
    "        print(f'Average response time for correct answers: {responseCorrect:.2f}')\n",
    "        responseIncorrect = trial[trial.iloc[:, 13] == 0].iloc[:, 14].mean()\n",
    "        print(f'Average response time for incorrect answers: {responseIncorrect:.2f}')\n",
    "        medianResponseTime = trial.iloc[:, 14].median()\n",
    "        print(f'Median response time: {medianResponseTime:.2f}')\n",
    "        medianResponseCorrect = trial[trial.iloc[:, 13] == 1].iloc[:, 14].median()\n",
    "        print(f'Median response time for correct answers: {medianResponseCorrect:.2f}')\n",
    "        medianResponseIncorrect = trial[trial.iloc[:, 13] == 0].iloc[:, 14].median()\n",
    "        print(f'Median response time for incorrect answers: {medianResponseIncorrect:.2f}')\n",
    "\n",
    "nbScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: 1001_NumericalSpeed.txt\n",
      "\n",
      "Scoring PRACT\n",
      "Proportion correct: 94.87% with total inputs: 39\n",
      "Average response time: 1036.67\n",
      "Average response time for correct answers: 1021.03\n",
      "Average response time for incorrect answers: 1326.00\n",
      "Median response time: 993.00\n",
      "Median response time for correct answers: 978.00\n",
      "Median response time for incorrect answers: 1326.00\n",
      "\n",
      "Scoring TRIAL1\n",
      "Proportion correct: 95.00% with total inputs: 40\n",
      "Average response time: 1050.78\n",
      "Average response time for correct answers: 1062.18\n",
      "Average response time for incorrect answers: 834.00\n",
      "Median response time: 1014.00\n",
      "Median response time for correct answers: 1026.00\n",
      "Median response time for incorrect answers: 834.00\n",
      "\n",
      "Scoring TRIAL2\n",
      "Proportion correct: 100.00% with total inputs: 40\n",
      "Average response time: 1031.97\n",
      "Average response time for correct answers: 1031.97\n",
      "Average response time for incorrect answers: nan\n",
      "Median response time: 978.00\n",
      "Median response time for correct answers: 978.00\n",
      "Median response time for incorrect answers: nan\n"
     ]
    }
   ],
   "source": [
    "def nsScore():\n",
    "    \n",
    "    # grab test document\n",
    "    ns = myUtil.findFile('NumericalSpeed')\n",
    "\n",
    "    # determine all unique trials\n",
    "    trials = myUtil.getTrialType(ns, 4)\n",
    "\n",
    "    # subset data by trial\n",
    "    trialDfs = myUtil.subsetByTrial(ns, trials)\n",
    "\n",
    "    # score each trial (this will be general scores)\n",
    "    for trial in trialDfs:\n",
    "        print(f'\\nScoring {trial.iloc[0, 4]}')\n",
    "        pc = trial.iloc[:, 13].mean()\n",
    "        print(f'Proportion correct: {pc:.2%} with total inputs: {trial.iloc[:, 13].count()}')\n",
    "        responseTimes = trial.iloc[:, 14].mean()\n",
    "        print(f'Average response time: {responseTimes:.2f}')\n",
    "        responseCorrect = trial[trial.iloc[:, 13] == 1].iloc[:, 14].mean()\n",
    "        print(f'Average response time for correct answers: {responseCorrect:.2f}')\n",
    "        responseIncorrect = trial[trial.iloc[:, 13] == 0].iloc[:, 14].mean()\n",
    "        print(f'Average response time for incorrect answers: {responseIncorrect:.2f}')\n",
    "        medianResponseTime = trial.iloc[:, 14].median()\n",
    "        print(f'Median response time: {medianResponseTime:.2f}')\n",
    "        medianResponseCorrect = trial[trial.iloc[:, 13] == 1].iloc[:, 14].median()\n",
    "        print(f'Median response time for correct answers: {medianResponseCorrect:.2f}')\n",
    "        medianResponseIncorrect = trial[trial.iloc[:, 13] == 0].iloc[:, 14].median()\n",
    "        print(f'Median response time for incorrect answers: {medianResponseIncorrect:.2f}')\n",
    "\n",
    "nsScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: 1001_SpeedTabbing.txt\n",
      "\n",
      "Scoring PRACT1\n",
      "Total number of presses: 158\n",
      "Average response time: 123.00\n",
      "Median response time: 120.00\n",
      "First press (ms): 120.00 and last press (ms): 128.00\n",
      "\n",
      "Scoring PRACT2\n",
      "Total number of presses: 130\n",
      "Average response time: 152.59\n",
      "Median response time: 151.00\n",
      "First press (ms): 253.00 and last press (ms): 127.00\n",
      "\n",
      "Scoring TRIAL1\n",
      "Total number of presses: 156\n",
      "Average response time: 127.56\n",
      "Median response time: 127.00\n",
      "First press (ms): 77.00 and last press (ms): 119.00\n",
      "\n",
      "Scoring TRIAL2\n",
      "Total number of presses: 136\n",
      "Average response time: 146.21\n",
      "Median response time: 136.00\n",
      "First press (ms): 87.00 and last press (ms): 167.00\n"
     ]
    }
   ],
   "source": [
    "def stScore():\n",
    "    \n",
    "    # grab test document\n",
    "    st = myUtil.findFile('SpeedTabbing')\n",
    "\n",
    "    # determine all unique trials\n",
    "    trials = myUtil.getTrialType(st)\n",
    "\n",
    "    # subset data by trial\n",
    "    trialDfs = myUtil.subsetByTrial(st, trials)\n",
    "\n",
    "    # score each trial (this will be general scores)\n",
    "    for trial in trialDfs:\n",
    "        print(f'\\nScoring {trial.iloc[0, 4]}')\n",
    "        numPresses = trial.iloc[:, 5].count()\n",
    "        print(f'Total number of presses: {numPresses}')\n",
    "        responseTimes = trial.iloc[:, 10].mean()\n",
    "        print(f'Average response time: {responseTimes:.2f}')\n",
    "        medianResponseTime = trial.iloc[:, 10].median()\n",
    "        print(f'Median response time: {medianResponseTime:.2f}')\n",
    "        firstPress = trial.iloc[0, 10]\n",
    "        lastPress = trial.iloc[-1, 10]\n",
    "        print(f'First press (ms): {firstPress:.2f} and last press (ms): {lastPress:.2f}')\n",
    "        \n",
    "stScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: 1001_VerbalSpeed.txt\n",
      "\n",
      "Scoring PRACT\n",
      "Proportion correct: 100.00% with total inputs: 39\n",
      "Average response time: 1053.38\n",
      "Average response time for correct answers: 1053.38\n",
      "Average response time for incorrect answers: nan\n",
      "Median response time: 1026.00\n",
      "Median response time for correct answers: 1026.00\n",
      "Median response time for incorrect answers: nan\n",
      "\n",
      "Scoring TRIAL1\n",
      "Proportion correct: 97.50% with total inputs: 40\n",
      "Average response time: 1035.05\n",
      "Average response time for correct answers: 1038.38\n",
      "Average response time for incorrect answers: 905.00\n",
      "Median response time: 1014.50\n",
      "Median response time for correct answers: 1034.00\n",
      "Median response time for incorrect answers: 905.00\n",
      "\n",
      "Scoring TRIAL2\n",
      "Proportion correct: 100.00% with total inputs: 40\n",
      "Average response time: 950.80\n",
      "Average response time for correct answers: 950.80\n",
      "Average response time for incorrect answers: nan\n",
      "Median response time: 910.00\n",
      "Median response time for correct answers: 910.00\n",
      "Median response time for incorrect answers: nan\n"
     ]
    }
   ],
   "source": [
    "def vsScore():\n",
    "    \n",
    "    # grab test document\n",
    "    vs = myUtil.findFile('VerbalSpeed')\n",
    "\n",
    "    # determine all unique trials\n",
    "    trials = myUtil.getTrialType(vs)\n",
    "\n",
    "    # subset data by trial\n",
    "    trialDfs = myUtil.subsetByTrial(vs, trials)\n",
    "\n",
    "    # score each trial (this will be general scores)\n",
    "    for trial in trialDfs:\n",
    "        print(f'\\nScoring {trial.iloc[0, 4]}')\n",
    "        pc = trial.iloc[:, 13].mean()\n",
    "        print(f'Proportion correct: {pc:.2%} with total inputs: {trial.iloc[:, 13].count()}')\n",
    "        responseTimes = trial.iloc[:, 14].mean()\n",
    "        print(f'Average response time: {responseTimes:.2f}')\n",
    "        responseCorrect = trial[trial.iloc[:, 13] == 1].iloc[:, 14].mean()\n",
    "        print(f'Average response time for correct answers: {responseCorrect:.2f}')\n",
    "        responseIncorrect = trial[trial.iloc[:, 13] == 0].iloc[:, 14].mean()\n",
    "        print(f'Average response time for incorrect answers: {responseIncorrect:.2f}')\n",
    "        medianResponseTime = trial.iloc[:, 14].median()\n",
    "        print(f'Median response time: {medianResponseTime:.2f}')\n",
    "        medianResponseCorrect = trial[trial.iloc[:, 13] == 1].iloc[:, 14].median()\n",
    "        print(f'Median response time for correct answers: {medianResponseCorrect:.2f}')\n",
    "        medianResponseIncorrect = trial[trial.iloc[:, 13] == 0].iloc[:, 14].median()\n",
    "        print(f'Median response time for incorrect answers: {medianResponseIncorrect:.2f}')\n",
    "        \n",
    "vsScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: 1001_ObjectLocationMemory.txt\n",
      "['TRIAL1' 'TRIAL2']\n",
      "\n",
      "Scoring TRIAL1\n",
      "Proportion correct: 7 of 12\n",
      "Total response time: 159040.00 ms\n",
      "\n",
      "Scoring TRIAL2\n",
      "Proportion correct: 6 of 12\n",
      "Total response time: 148840.00 ms\n"
     ]
    }
   ],
   "source": [
    "def olmScore():\n",
    "\n",
    "    # NOTE: consists of 6x6 grid of objects\n",
    "    \n",
    "    # grab test document\n",
    "    olm = myUtil.findFile('ObjectLocationMemory')\n",
    "\n",
    "    # determine all unique trials\n",
    "    trials = myUtil.getTrialType(olm)\n",
    "    print(trials)\n",
    "    \n",
    "    # subset data by trial\n",
    "    trialDfs = myUtil.subsetByTrial(olm, trials)\n",
    "\n",
    "    # score each trial (this will be general scores)\n",
    "    for trial in trialDfs:\n",
    "        print(f'\\nScoring {trial.iloc[0, 4]}')\n",
    "        pc = trial.iloc[:, 9].item()\n",
    "        print(f'Proportion correct: {pc} of 12')\n",
    "        responseTimes = trial.iloc[:, 7].item()\n",
    "        print(f'Total response time: {responseTimes:.2f} ms')\n",
    "\n",
    "    # Nitty gritty details\n",
    "    # 1. Determine the original coordinates of the object\n",
    "    # 2. Determine the coordinates of the object after the translation\n",
    "\n",
    "olmScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: 1001_SpatialUpdating.txt\n",
      "['PRACT' 'TRIAL1' 'TRIAL2']\n",
      "\n",
      "Scoring PRACT\n",
      "Proportion correct for grid 1: 100.00% with total inputs: 4\n",
      "Proportion correct for grid 2: 25.00% with total inputs: 4\n",
      "Proportion correct for grid 3: 50.00% with total inputs: 4\n",
      "Proportion correct for all grids correct: 0.00% with total inputs: 4\n",
      "\n",
      "Scoring TRIAL1\n",
      "Proportion correct for grid 1: 80.00% with total inputs: 5\n",
      "Proportion correct for grid 2: 40.00% with total inputs: 5\n",
      "Proportion correct for grid 3: 20.00% with total inputs: 5\n",
      "Proportion correct for all grids correct: 20.00% with total inputs: 5\n",
      "\n",
      "Scoring TRIAL2\n",
      "Proportion correct for grid 1: 40.00% with total inputs: 5\n",
      "Proportion correct for grid 2: 40.00% with total inputs: 5\n",
      "Proportion correct for grid 3: 40.00% with total inputs: 5\n",
      "Proportion correct for all grids correct: 0.00% with total inputs: 5\n"
     ]
    }
   ],
   "source": [
    "def suScore():\n",
    "    # NOTE: consists of 3, 3x3 grids of objects\n",
    "    \n",
    "    # grab test document\n",
    "    su = myUtil.findFile('SpatialUpdating')\n",
    "\n",
    "    # determine all unique trials\n",
    "    trials = myUtil.getTrialType(su)\n",
    "    print(trials)\n",
    "\n",
    "    # subset data by trial\n",
    "    trialDfs = myUtil.subsetByTrial(su, trials)\n",
    "\n",
    "    # score each trial (this will be general scores)\n",
    "    for trial in trialDfs:\n",
    "        print(f'\\nScoring {trial.iloc[0, 4]}')\n",
    "        pcGrid1 = trial.iloc[:, 33].mean()\n",
    "        pcGrid2 = trial.iloc[:, 34].mean()\n",
    "        pcGrid3 = trial.iloc[:, 35].mean()\n",
    "        pcTotal = trial.iloc[:, 36].mean()\n",
    "        print(f'Proportion correct for grid 1: {pcGrid1:.2%} with total inputs: {trial.iloc[:, 33].count()}')\n",
    "        print(f'Proportion correct for grid 2: {pcGrid2:.2%} with total inputs: {trial.iloc[:, 34].count()}')\n",
    "        print(f'Proportion correct for grid 3: {pcGrid3:.2%} with total inputs: {trial.iloc[:, 35].count()}')\n",
    "        print(f'Proportion correct for all grids correct: {pcTotal:.2%} with total inputs: {trial.iloc[:, 36].count()}')\n",
    "        \n",
    "\n",
    "    # Nitty gritty details\n",
    "    # 1. Determine the original coordinates of the object\n",
    "    # 2. Determine the coordinates of the object after the translation\n",
    "    \n",
    "suScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully read file: 1001_WordRecall.txt\n",
      "\n",
      "Scoring TRIAL1\n",
      "Proportion correct: 100.00% with total responses: 11\n",
      "Proportion of intrusions: 0.00% with total responses: 11\n",
      "\n",
      "Scoring TRIAL2\n",
      "Proportion correct: 88.89% with total responses: 9\n",
      "Proportion of intrusions: 11.11% with total responses: 9\n"
     ]
    }
   ],
   "source": [
    "def wrScore():\n",
    "    \n",
    "    # grab test document\n",
    "    wr = myUtil.findFile('WordRecall')\n",
    "\n",
    "    # determine all unique trials\n",
    "    trials = myUtil.getTrialType(wr)\n",
    "\n",
    "    # subset data by trial\n",
    "    trialDfs = myUtil.subsetByTrial(wr, trials)\n",
    "\n",
    "    # score each trial (this will be general scores)\n",
    "    for trial in trialDfs:\n",
    "        print(f'\\nScoring {trial.iloc[0, 4]}')\n",
    "        pc = trial.iloc[:, 24].item() / trial.iloc[:, 23].item()\n",
    "        print(f'Proportion correct: {pc:.2%} with total responses: {trial.iloc[:, 23].item()}')\n",
    "        intrusions = trial.iloc[:, 25].item() / trial.iloc[:, 23].item()\n",
    "        print(f'Proportion of intrusions: {intrusions:.2%} with total responses: {trial.iloc[:, 23].item()}')\n",
    "        \n",
    "\n",
    "wrScore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created directory: /Users/ethanjfranco/Desktop/GitHub/naftaliFiles/CognitiveTests/1001/scored\n",
      "Successfully moved lu_scores.csv to /Users/ethanjfranco/Desktop/GitHub/naftaliFiles/CognitiveTests/1001/scored\n",
      "Successfully moved fs_scores.csv to /Users/ethanjfranco/Desktop/GitHub/naftaliFiles/CognitiveTests/1001/scored\n"
     ]
    }
   ],
   "source": [
    "myUtil.makeScoredDir()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
